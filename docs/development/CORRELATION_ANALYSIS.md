# Trace-Log Correlation Analysis Report

**Date**: December 28, 2025  
**Service**: rust-datadog-otel  
**Environment**: development  

## üîç Executive Summary

**Status: ‚ùå Trace-Log Correlation is NOT Working**

While both traces and logs are successfully being collected by Datadog, they are **NOT correlated**. The logs do not contain the `dd.trace_id` and `dd.span_id` attributes required for Datadog to link logs with traces.

## üìä Findings from Datadog

### ‚úÖ What's Working

**1. APM Traces**
- ‚úì Traces are successfully collected
- ‚úì Spans show proper hierarchy
- ‚úì Service tagging is correct (service:rust-datadog-otel)
- ‚úì Error tracking works
- ‚úì Performance metrics available
- ‚úì Source: `apm` (from OpenTelemetry SDK)

**Example Trace:**
```yaml
span_id: "10876861900508630237"
trace_id: "695103ee00000000b1cf9c8f6c726ec4"
service: rust-datadog-otel
resource: simulate_error
status: error
tags:
  - ingestion_reason:otel
  - env:development
  - version:0.1.0
```

**2. Container Logs**
- ‚úì Logs are successfully collected
- ‚úì JSON format is parsed correctly
- ‚úì Service tagging is correct
- ‚úì Log levels (ERROR, INFO, DEBUG) work
- ‚úì Source: `rust` (container logs)

**Example Log:**
```yaml
service: rust-datadog-otel
status: error
attributes:
  level: ERROR
  span:
    name: simulate_error
    params: 'ErrorSimulationQuery { error_type: "server" }'
  spans:
    - name: simulate_error
      params: 'ErrorSimulationQuery { error_type: "server" }'
  target: rust_datadog_otel
  threadId: ThreadId(2)
  threadName: tokio-runtime-worker
```

### ‚ùå What's NOT Working

**Missing Correlation Fields**

The logs **DO NOT** contain:
- ‚ùå `dd.trace_id` (Datadog trace ID in decimal format)
- ‚ùå `dd.span_id` (Datadog span ID in decimal format)

**Why This Matters:**

Datadog uses these specific fields to correlate logs with traces. Without them:
- Logs won't show up in trace views
- Traces won't show up in log views  
- No "View Trace" button in logs
- No "Logs" tab in traces
- Cannot navigate between logs and traces

## üîç Root Cause Analysis

### Current Log Format

The application outputs span context like this:

```json
{
  "level": "ERROR",
  "timestamp": "2025-12-28T10:19:12.139456Z",
  "span": {
    "name": "simulate_error",
    "params": "ErrorSimulationQuery { error_type: \"server\" }"
  },
  "spans": [
    {
      "name": "simulate_error",
      "params": "ErrorSimulationQuery { error_type: \"server\" }"
    }
  ]
}
```

**Problems:**
1. Span context is in a nested, descriptive format
2. No trace_id or span_id fields at all (even in hex format)
3. Only span names are included, not IDs
4. Format is not recognized by Datadog Agent for correlation

### Expected Format for Correlation

Datadog needs logs in this format:

```json
{
  "level": "ERROR",
  "timestamp": "2025-12-28T10:19:12.139456Z",
  "message": "Simulating error",
  "dd.trace_id": "7621889953578991300",     // ‚Üê Required (decimal)
  "dd.span_id": "10876861900508630237",      // ‚Üê Required (decimal)
  "dd.service": "rust-datadog-otel",
  "dd.env": "development",
  "dd.version": "0.1.0"
}
```

Or these can be extracted from OpenTelemetry format:

```json
{
  "level": "ERROR",
  "trace_id": "0x695103ee00000000b1cf9c8f6c726ec4",  // OTel hex
  "span_id": "0x970e8e3abfcb39bd"                    // OTel hex
}
```

## üõ†Ô∏è Why This Is Happening

### Issue: tracing-opentelemetry Limitation

The `tracing-opentelemetry` crate's current implementation:

```rust
// From src/telemetry.rs
tracing_subscriber::fmt::layer()
    .json()
    .with_current_span(true)     // ‚Üê Adds span info
    .with_span_list(true)         // ‚Üê Adds span hierarchy
```

**What this does:**
- ‚úì Adds span context to logs
- ‚úì Includes span names and fields
- ‚ùå Does NOT include trace_id or span_id
- ‚ùå Format is not Datadog-compatible

### Why trace_id/span_id Are Missing

The `with_current_span(true)` option includes span metadata (name, fields) but does **NOT** include the OpenTelemetry trace context (trace_id, span_id). This is a known limitation of the `tracing-subscriber` JSON formatter.

## üí° Possible Solutions

### Option 1: Custom Logging Layer (Recommended)

Create a custom `tracing_subscriber` layer that extracts trace IDs from the OpenTelemetry context and adds them to logs.

**Pros:**
- ‚úì Most control over format
- ‚úì Can output exactly what Datadog needs
- ‚úì Works with existing setup

**Cons:**
- ‚ùå Requires custom code
- ‚ùå Maintenance overhead
- ‚ùå More complexity

**Implementation:**
```rust
// Would need to create a custom layer that:
// 1. Extracts current OTel trace context
// 2. Converts trace_id/span_id to decimal format
// 3. Adds dd.trace_id and dd.span_id to log output
```

### Option 2: Datadog Agent Log Processing

Configure the Datadog Agent to parse OpenTelemetry trace IDs from container logs.

**Current Investigation Needed:**
- Check if Agent can parse nested JSON fields
- Verify Agent configuration for OTel trace ID extraction
- Test custom log pipelines in Datadog

**Pros:**
- ‚úì No application code changes
- ‚úì Centralized configuration

**Cons:**
- ‚ùå Requires Agent configuration changes
- ‚ùå May not work with current log format
- ‚ùå Need to verify Agent capabilities

### Option 3: Structured Trace ID Logging

Modify the application to explicitly log trace IDs:

```rust
use opentelemetry::trace::TraceContextExt;

#[instrument]
async fn handler() -> impl IntoResponse {
    // Get current span context
    let context = tracing::Span::current().context();
    let span_context = context.span().span_context();
    
    // Log with trace IDs
    info!(
        trace_id = %span_context.trace_id(),
        span_id = %span_context.span_id(),
        "Processing request"
    );
    
    // ... handler logic
}
```

**Pros:**
- ‚úì Explicit control over trace ID logging
- ‚úì Can format as needed

**Cons:**
- ‚ùå Requires changes to every handler
- ‚ùå Repetitive code
- ‚ùå Easy to forget in new handlers

### Option 4: Use Datadog Logging Library

Use Datadog's logging library alongside OpenTelemetry tracing.

**Pros:**
- ‚úì Native Datadog support
- ‚úì Automatic correlation

**Cons:**
- ‚ùå Vendor lock-in
- ‚ùå Requires additional dependency
- ‚ùå May not integrate well with tracing ecosystem

## üìã Recommended Action Plan

### Short Term: Verify Current Behavior

1. ‚úÖ **Confirmed**: Logs do NOT have dd.trace_id/dd.span_id
2. ‚úÖ **Confirmed**: Traces are collected successfully
3. ‚úÖ **Confirmed**: Logs are collected successfully
4. ‚ùå **Confirmed**: No correlation exists

### Medium Term: Implement Solution

**Recommended: Option 1 (Custom Logging Layer)**

1. Create a custom `tracing_subscriber` layer
2. Extract OpenTelemetry context in the layer
3. Add `dd.trace_id` and `dd.span_id` to log output
4. Convert IDs from hex to decimal format
5. Test correlation in Datadog UI

**Estimated Effort**: 4-8 hours

### Long Term: Monitor and Maintain

1. Test correlation with various log levels
2. Verify correlation across distributed requests
3. Document the implementation
4. Consider contributing back to tracing-opentelemetry

## üîó References

### Datadog Documentation
- [Connect Logs and Traces](https://docs.datadoghq.com/tracing/other_telemetry/connect_logs_and_traces/)
- [Rust Tracing](https://docs.datadoghq.com/tracing/trace_collection/custom_instrumentation/rust)
- [Log Management](https://docs.datadoghq.com/logs/)

### Rust Crates
- [tracing-opentelemetry](https://docs.rs/tracing-opentelemetry/)
- [tracing-subscriber](https://docs.rs/tracing-subscriber/)
- [opentelemetry](https://docs.rs/opentelemetry/)

### Related Issues
- [tracing-opentelemetry: Add trace_id to JSON logs](https://github.com/tokio-rs/tracing-opentelemetry/issues/)
- [OpenTelemetry Specification: Trace Context](https://opentelemetry.io/docs/specs/otel/trace/api/#spancontext)

## üìä Data Summary

### Logs Collected (Last Hour)
- **Count**: 1,237 logs
- **Services**: rust-datadog-otel
- **Environments**: development
- **Hosts**: 2 pods
- **Status**: ‚úì Successfully collected
- **Correlation**: ‚ùå Not correlated

### Traces Collected (Last Hour)
- **Count**: 493 spans
- **Services**: rust-datadog-otel
- **Environments**: development
- **Status**: ‚úì Successfully collected
- **Error Tracking**: ‚úì Working
- **Correlation**: ‚ùå Not available for logs

## ‚úÖ Next Steps

1. **Immediate**: Share this analysis with the team
2. **This Week**: Prototype custom logging layer
3. **Next Week**: Test and validate correlation
4. **Following Week**: Roll out to production
5. **Ongoing**: Monitor correlation metrics

---

**Report Generated**: December 28, 2025  
**Data Source**: Datadog MCP Tools  
**Analysis Tool**: Cursor AI Assistant

